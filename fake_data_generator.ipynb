{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for Generate Fake Data using CTGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDependencies\\n- SDV\\n- SDMetrics\\n- TableEvaluator\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dependencies\n",
    "- SDV\n",
    "- SDMetrics\n",
    "- TableEvaluator\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.metadata import SingleTableMetadata\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "from sdmetrics.reports.single_table import QualityReport\n",
    "from table_evaluator import TableEvaluator\n",
    "    \n",
    "def make_metadata(real_data):\n",
    "    \"\"\"\n",
    "        Create metadata for SDV data needs\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        real_data : DataFrame\n",
    "            real data in Pandas dataframe\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        object\n",
    "            metadata as object. It can be converted to dictionary\n",
    "\n",
    "    \"\"\"\n",
    "    meta = SingleTableMetadata()\n",
    "    meta.detect_from_dataframe(real_data)\n",
    "\n",
    "    return meta\n",
    "\n",
    "def make_fake_ctgan(metadata, epoch, real_data, fake_nums, verbose=True):\n",
    "    \"\"\"\n",
    "        Generate fake data via CTGAN\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        metadata : object\n",
    "            SDV metadata\n",
    "        epoch : int\n",
    "            number of epoch\n",
    "        real_data : DataFrame\n",
    "            real data in Pandas dataframe\n",
    "        fake_nums : int\n",
    "            numbers of generated fake data\n",
    "        verbose : bool\n",
    "            verbose mode. Default 'True'\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        DataFrame\n",
    "            Fake data as DataFrame\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define CTGAN Model\n",
    "    ctgan_model = CTGANSynthesizer(\n",
    "        metadata=metadata,\n",
    "        enforce_rounding=False,\n",
    "        epochs=epoch,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Train CTGAN\n",
    "    ctgan_model.fit(real_data)\n",
    "\n",
    "    # Generate Fake Data\n",
    "    fake_data = ctgan_model.sample(fake_nums)\n",
    "\n",
    "    return fake_data\n",
    "\n",
    "def fake_data_quality(real_data, fake_data, metadata):\n",
    "    \"\"\"\n",
    "        Check fake data quality using SDMetrics\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        real_data : DataFrame\n",
    "            real data as Pandas dataframe\n",
    "        fake_data : DataFrame\n",
    "            fake data as Pandas dataframe\n",
    "        meta : Object\n",
    "            SDV metadata object\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        void\n",
    "            print quality report\n",
    "    \"\"\"\n",
    "\n",
    "    report = QualityReport()\n",
    "    report.generate(\n",
    "        real_data=real_data,\n",
    "        synthetic_data=fake_data,\n",
    "        metadata=metadata.to_dict()\n",
    "    )\n",
    "\n",
    "def fake_data_table_evaluator(real_data, fake_data, target_col):\n",
    "    \"\"\"\n",
    "        Evaluate fake data quality using Table Evaluator\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        real_data : DataFrame\n",
    "            real data as Pandas dataframe\n",
    "        fake_data : DataFrame\n",
    "            fake data as Pandas dataframe\n",
    "        target_col : str\n",
    "            class column' name\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        void\n",
    "            print evaluation report\n",
    "    \"\"\"\n",
    "    eval = TableEvaluator(real=real_data, fake=fake_data)\n",
    "    eval.evaluate(target_col=target_col)\n",
    "\n",
    "\n",
    "def export_data(fake_data, fake_data_path):\n",
    "    \"\"\"\n",
    "        Export fake data as csv\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        fake_data : DataFrame\n",
    "            fake data in Pandas dataframe format\n",
    "        fake_data_path : str\n",
    "            data path with extension\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        void\n",
    "            print success message\n",
    "    \"\"\"\n",
    "    fake_data.to_csv(fake_data_path, index=False)\n",
    "\n",
    "    return \"Success!\"\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Generator Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>P3</th>\n",
       "      <th>P4</th>\n",
       "      <th>P5</th>\n",
       "      <th>P6</th>\n",
       "      <th>P7</th>\n",
       "      <th>P8</th>\n",
       "      <th>P9</th>\n",
       "      <th>P10</th>\n",
       "      <th>P11</th>\n",
       "      <th>P12</th>\n",
       "      <th>P13</th>\n",
       "      <th>P14</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   P1  P2  P3  P4  P5  P6  P7  P8  P9  P10  P11  P12  P13  P14  CLASS\n",
       "0   3   2   0   3   3   2   0   1   2    1    2    1    2    1      1\n",
       "1   3   2   0   2   2   3   0   2   2    1    3    1    3    1      1\n",
       "2   1   2   0   2   1   2   0   1   1    1    2    1    1    1      1\n",
       "3   2   1   0   2   0   1   0   2   1    1    0    1    1    1      1\n",
       "4   2   3   2   2   3   2   2   1   1    0    2    0    3    1      2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_data = pd.read_csv('../anxiety_class_encoded.csv')\n",
    "real_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metadata\n",
    "meta = make_metadata(real_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G:  0.0643,Loss D:  0.0028\n",
      "Epoch 2, Loss G:  0.0770,Loss D: -0.0363\n",
      "Epoch 3, Loss G:  0.0771,Loss D: -0.0802\n",
      "Epoch 4, Loss G:  0.0663,Loss D: -0.0695\n",
      "Epoch 5, Loss G:  0.0759,Loss D: -0.1288\n",
      "Epoch 6, Loss G:  0.0556,Loss D: -0.0995\n",
      "Epoch 7, Loss G:  0.0578,Loss D: -0.1446\n",
      "Epoch 8, Loss G:  0.0523,Loss D: -0.1775\n",
      "Epoch 9, Loss G:  0.0427,Loss D: -0.1805\n",
      "Epoch 10, Loss G:  0.0145,Loss D: -0.2177\n",
      "Epoch 11, Loss G: -0.0362,Loss D: -0.2186\n",
      "Epoch 12, Loss G: -0.0220,Loss D: -0.2454\n",
      "Epoch 13, Loss G: -0.0678,Loss D: -0.2582\n",
      "Epoch 14, Loss G: -0.0699,Loss D: -0.3264\n",
      "Epoch 15, Loss G: -0.1310,Loss D: -0.4417\n",
      "Epoch 16, Loss G: -0.2032,Loss D: -0.3842\n",
      "Epoch 17, Loss G: -0.2359,Loss D: -0.4780\n",
      "Epoch 18, Loss G: -0.2997,Loss D: -0.4018\n",
      "Epoch 19, Loss G: -0.4252,Loss D: -0.4708\n",
      "Epoch 20, Loss G: -0.4488,Loss D: -0.5603\n",
      "Epoch 21, Loss G: -0.5128,Loss D: -0.6153\n",
      "Epoch 22, Loss G: -0.5364,Loss D: -0.6499\n",
      "Epoch 23, Loss G: -0.6515,Loss D: -0.6779\n",
      "Epoch 24, Loss G: -0.6772,Loss D: -0.6872\n",
      "Epoch 25, Loss G: -0.7510,Loss D: -0.7054\n",
      "Epoch 26, Loss G: -0.9113,Loss D: -0.6723\n",
      "Epoch 27, Loss G: -0.9654,Loss D: -0.5689\n",
      "Epoch 28, Loss G: -0.8306,Loss D: -0.6053\n",
      "Epoch 29, Loss G: -0.9651,Loss D: -0.8236\n",
      "Epoch 30, Loss G: -1.1078,Loss D: -0.5519\n",
      "Epoch 31, Loss G: -1.0585,Loss D: -0.5824\n",
      "Epoch 32, Loss G: -1.1768,Loss D: -0.4775\n",
      "Epoch 33, Loss G: -1.2128,Loss D: -0.6330\n",
      "Epoch 34, Loss G: -1.2211,Loss D: -0.4497\n",
      "Epoch 35, Loss G: -1.2706,Loss D: -0.3443\n",
      "Epoch 36, Loss G: -1.2861,Loss D: -0.2643\n",
      "Epoch 37, Loss G: -1.4920,Loss D: -0.2341\n",
      "Epoch 38, Loss G: -1.4428,Loss D: -0.4198\n",
      "Epoch 39, Loss G: -1.4246,Loss D: -0.2233\n",
      "Epoch 40, Loss G: -1.4384,Loss D: -0.2816\n",
      "Epoch 41, Loss G: -1.6391,Loss D: -0.1166\n",
      "Epoch 42, Loss G: -1.5031,Loss D: -0.1421\n",
      "Epoch 43, Loss G: -1.4861,Loss D: -0.1829\n",
      "Epoch 44, Loss G: -1.5652,Loss D: -0.2461\n",
      "Epoch 45, Loss G: -1.4174,Loss D: -0.3166\n",
      "Epoch 46, Loss G: -1.3456,Loss D: -0.0599\n",
      "Epoch 47, Loss G: -1.3626,Loss D: -0.2196\n",
      "Epoch 48, Loss G: -1.3662,Loss D: -0.0790\n",
      "Epoch 49, Loss G: -1.4351,Loss D: -0.1725\n",
      "Epoch 50, Loss G: -1.2008,Loss D: -0.2053\n",
      "Epoch 51, Loss G: -1.2481,Loss D: -0.1767\n",
      "Epoch 52, Loss G: -1.2030,Loss D: -0.2204\n",
      "Epoch 53, Loss G: -1.2992,Loss D: -0.1399\n",
      "Epoch 54, Loss G: -1.1075,Loss D: -0.2084\n",
      "Epoch 55, Loss G: -1.1679,Loss D: -0.2568\n",
      "Epoch 56, Loss G: -1.1488,Loss D: -0.1704\n",
      "Epoch 57, Loss G: -1.1341,Loss D: -0.2052\n",
      "Epoch 58, Loss G: -1.1727,Loss D: -0.3212\n",
      "Epoch 59, Loss G: -1.1315,Loss D: -0.1083\n",
      "Epoch 60, Loss G: -1.1122,Loss D: -0.1559\n",
      "Epoch 61, Loss G: -1.1777,Loss D: -0.1062\n",
      "Epoch 62, Loss G: -1.1667,Loss D: -0.0779\n",
      "Epoch 63, Loss G: -1.1114,Loss D:  0.0005\n",
      "Epoch 64, Loss G: -1.1724,Loss D: -0.1858\n",
      "Epoch 65, Loss G: -1.1629,Loss D: -0.1661\n",
      "Epoch 66, Loss G: -1.2052,Loss D: -0.0624\n",
      "Epoch 67, Loss G: -1.1233,Loss D:  0.0770\n",
      "Epoch 68, Loss G: -1.0402,Loss D: -0.1724\n",
      "Epoch 69, Loss G: -1.0853,Loss D: -0.0755\n",
      "Epoch 70, Loss G: -1.1694,Loss D: -0.1640\n",
      "Epoch 71, Loss G: -1.1098,Loss D: -0.1590\n",
      "Epoch 72, Loss G: -1.0830,Loss D: -0.0576\n",
      "Epoch 73, Loss G: -1.0682,Loss D: -0.1769\n",
      "Epoch 74, Loss G: -1.1724,Loss D: -0.1021\n",
      "Epoch 75, Loss G: -1.0954,Loss D: -0.2119\n",
      "Epoch 76, Loss G: -0.9559,Loss D: -0.1279\n",
      "Epoch 77, Loss G: -0.9120,Loss D: -0.3230\n",
      "Epoch 78, Loss G: -0.9576,Loss D: -0.2137\n",
      "Epoch 79, Loss G: -0.9490,Loss D: -0.2288\n",
      "Epoch 80, Loss G: -1.0461,Loss D: -0.3790\n",
      "Epoch 81, Loss G: -0.9811,Loss D: -0.2394\n",
      "Epoch 82, Loss G: -0.9521,Loss D: -0.2140\n",
      "Epoch 83, Loss G: -1.0011,Loss D: -0.1725\n",
      "Epoch 84, Loss G: -0.9285,Loss D: -0.1114\n",
      "Epoch 85, Loss G: -0.9142,Loss D: -0.1746\n",
      "Epoch 86, Loss G: -1.0612,Loss D: -0.0945\n",
      "Epoch 87, Loss G: -1.0909,Loss D: -0.1311\n",
      "Epoch 88, Loss G: -1.1844,Loss D:  0.0154\n",
      "Epoch 89, Loss G: -1.3100,Loss D:  0.0635\n",
      "Epoch 90, Loss G: -1.2545,Loss D:  0.1300\n",
      "Epoch 91, Loss G: -1.2817,Loss D:  0.0870\n",
      "Epoch 92, Loss G: -1.4100,Loss D:  0.2388\n",
      "Epoch 93, Loss G: -1.3541,Loss D:  0.1889\n",
      "Epoch 94, Loss G: -1.3576,Loss D:  0.1441\n",
      "Epoch 95, Loss G: -1.4651,Loss D:  0.2005\n",
      "Epoch 96, Loss G: -1.4000,Loss D:  0.0999\n",
      "Epoch 97, Loss G: -1.5327,Loss D:  0.0838\n",
      "Epoch 98, Loss G: -1.5589,Loss D:  0.0574\n",
      "Epoch 99, Loss G: -1.4148,Loss D:  0.0610\n",
      "Epoch 100, Loss G: -1.4765,Loss D:  0.1171\n",
      "Epoch 101, Loss G: -1.4180,Loss D:  0.2107\n",
      "Epoch 102, Loss G: -1.5749,Loss D:  0.0048\n",
      "Epoch 103, Loss G: -1.4165,Loss D: -0.0431\n",
      "Epoch 104, Loss G: -1.2974,Loss D:  0.0339\n",
      "Epoch 105, Loss G: -1.3477,Loss D: -0.0801\n",
      "Epoch 106, Loss G: -1.1687,Loss D:  0.0008\n",
      "Epoch 107, Loss G: -1.2308,Loss D: -0.1820\n",
      "Epoch 108, Loss G: -1.1868,Loss D: -0.1904\n",
      "Epoch 109, Loss G: -1.1379,Loss D: -0.1653\n",
      "Epoch 110, Loss G: -1.2140,Loss D: -0.2595\n",
      "Epoch 111, Loss G: -1.1378,Loss D: -0.1721\n",
      "Epoch 112, Loss G: -1.2446,Loss D: -0.1328\n",
      "Epoch 113, Loss G: -1.2354,Loss D: -0.2260\n",
      "Epoch 114, Loss G: -1.3003,Loss D: -0.2009\n",
      "Epoch 115, Loss G: -1.3447,Loss D: -0.1871\n",
      "Epoch 116, Loss G: -1.4277,Loss D: -0.0019\n",
      "Epoch 117, Loss G: -1.4428,Loss D: -0.0246\n",
      "Epoch 118, Loss G: -1.6486,Loss D: -0.0178\n",
      "Epoch 119, Loss G: -1.6441,Loss D:  0.0238\n",
      "Epoch 120, Loss G: -1.7224,Loss D:  0.2002\n",
      "Epoch 121, Loss G: -1.7890,Loss D:  0.1356\n",
      "Epoch 122, Loss G: -1.7968,Loss D:  0.3121\n",
      "Epoch 123, Loss G: -1.9666,Loss D:  0.1984\n",
      "Epoch 124, Loss G: -2.0645,Loss D:  0.3165\n",
      "Epoch 125, Loss G: -2.0725,Loss D:  0.2988\n",
      "Epoch 126, Loss G: -2.0623,Loss D:  0.2854\n",
      "Epoch 127, Loss G: -2.0925,Loss D:  0.2699\n",
      "Epoch 128, Loss G: -2.2594,Loss D:  0.2528\n",
      "Epoch 129, Loss G: -2.0774,Loss D:  0.1108\n",
      "Epoch 130, Loss G: -2.2398,Loss D:  0.2591\n",
      "Epoch 131, Loss G: -2.2167,Loss D:  0.1301\n",
      "Epoch 132, Loss G: -2.1177,Loss D:  0.1937\n",
      "Epoch 133, Loss G: -2.0210,Loss D:  0.1984\n",
      "Epoch 134, Loss G: -2.0802,Loss D:  0.0122\n",
      "Epoch 135, Loss G: -2.1380,Loss D: -0.2088\n",
      "Epoch 136, Loss G: -2.1551,Loss D:  0.1698\n",
      "Epoch 137, Loss G: -2.1715,Loss D:  0.0795\n",
      "Epoch 138, Loss G: -2.0626,Loss D:  0.0471\n",
      "Epoch 139, Loss G: -2.1236,Loss D:  0.0036\n",
      "Epoch 140, Loss G: -2.1259,Loss D: -0.0789\n",
      "Epoch 141, Loss G: -2.1022,Loss D:  0.1848\n",
      "Epoch 142, Loss G: -2.1824,Loss D:  0.0695\n",
      "Epoch 143, Loss G: -2.1232,Loss D:  0.0781\n",
      "Epoch 144, Loss G: -2.3776,Loss D:  0.1368\n",
      "Epoch 145, Loss G: -2.1868,Loss D:  0.1363\n",
      "Epoch 146, Loss G: -2.3199,Loss D:  0.0956\n",
      "Epoch 147, Loss G: -2.3789,Loss D:  0.1823\n",
      "Epoch 148, Loss G: -2.3545,Loss D:  0.0220\n",
      "Epoch 149, Loss G: -2.2860,Loss D: -0.1208\n",
      "Epoch 150, Loss G: -2.3891,Loss D:  0.2251\n",
      "Epoch 151, Loss G: -2.2081,Loss D:  0.0534\n",
      "Epoch 152, Loss G: -2.2733,Loss D:  0.0617\n",
      "Epoch 153, Loss G: -2.2078,Loss D:  0.0640\n",
      "Epoch 154, Loss G: -2.0445,Loss D: -0.0175\n",
      "Epoch 155, Loss G: -2.2072,Loss D: -0.1072\n",
      "Epoch 156, Loss G: -2.0789,Loss D: -0.0606\n",
      "Epoch 157, Loss G: -2.0722,Loss D: -0.1574\n",
      "Epoch 158, Loss G: -1.9327,Loss D: -0.1104\n",
      "Epoch 159, Loss G: -1.8855,Loss D: -0.1264\n",
      "Epoch 160, Loss G: -1.8746,Loss D: -0.2870\n",
      "Epoch 161, Loss G: -1.7786,Loss D: -0.1120\n",
      "Epoch 162, Loss G: -1.8612,Loss D: -0.0766\n",
      "Epoch 163, Loss G: -1.8545,Loss D: -0.0280\n",
      "Epoch 164, Loss G: -1.8001,Loss D: -0.0054\n",
      "Epoch 165, Loss G: -1.8027,Loss D:  0.0327\n",
      "Epoch 166, Loss G: -1.8668,Loss D: -0.0838\n",
      "Epoch 167, Loss G: -1.9494,Loss D: -0.0017\n",
      "Epoch 168, Loss G: -1.9435,Loss D:  0.0727\n",
      "Epoch 169, Loss G: -2.0355,Loss D:  0.0869\n",
      "Epoch 170, Loss G: -2.0333,Loss D:  0.1993\n",
      "Epoch 171, Loss G: -2.1604,Loss D:  0.0867\n",
      "Epoch 172, Loss G: -1.9897,Loss D:  0.2196\n",
      "Epoch 173, Loss G: -2.0879,Loss D:  0.1503\n",
      "Epoch 174, Loss G: -2.0161,Loss D:  0.0927\n",
      "Epoch 175, Loss G: -1.9743,Loss D: -0.0060\n",
      "Epoch 176, Loss G: -2.0362,Loss D: -0.0536\n",
      "Epoch 177, Loss G: -1.8162,Loss D: -0.0763\n",
      "Epoch 178, Loss G: -1.8218,Loss D: -0.2327\n",
      "Epoch 179, Loss G: -1.7422,Loss D: -0.0586\n",
      "Epoch 180, Loss G: -1.7614,Loss D: -0.3609\n",
      "Epoch 181, Loss G: -1.6818,Loss D: -0.2562\n",
      "Epoch 182, Loss G: -1.5927,Loss D: -0.4117\n",
      "Epoch 183, Loss G: -1.5504,Loss D: -0.3292\n",
      "Epoch 184, Loss G: -1.6126,Loss D: -0.2978\n",
      "Epoch 185, Loss G: -1.7022,Loss D: -0.3119\n",
      "Epoch 186, Loss G: -1.6701,Loss D: -0.1745\n",
      "Epoch 187, Loss G: -1.8478,Loss D: -0.0645\n",
      "Epoch 188, Loss G: -1.7902,Loss D: -0.0022\n",
      "Epoch 189, Loss G: -1.8958,Loss D:  0.0322\n",
      "Epoch 190, Loss G: -2.0091,Loss D:  0.1517\n",
      "Epoch 191, Loss G: -2.0382,Loss D:  0.1407\n",
      "Epoch 192, Loss G: -2.0183,Loss D:  0.2027\n",
      "Epoch 193, Loss G: -2.0072,Loss D:  0.1939\n",
      "Epoch 194, Loss G: -2.0740,Loss D:  0.3270\n",
      "Epoch 195, Loss G: -2.0699,Loss D:  0.3309\n",
      "Epoch 196, Loss G: -2.1072,Loss D:  0.2766\n",
      "Epoch 197, Loss G: -1.9877,Loss D:  0.0287\n",
      "Epoch 198, Loss G: -1.9353,Loss D:  0.1783\n",
      "Epoch 199, Loss G: -1.8950,Loss D: -0.1605\n",
      "Epoch 200, Loss G: -1.9215,Loss D:  0.0685\n",
      "Epoch 201, Loss G: -1.8420,Loss D: -0.0927\n",
      "Epoch 202, Loss G: -1.8438,Loss D: -0.0815\n",
      "Epoch 203, Loss G: -1.8195,Loss D: -0.0310\n",
      "Epoch 204, Loss G: -1.7746,Loss D:  0.0682\n",
      "Epoch 205, Loss G: -1.9271,Loss D: -0.0612\n",
      "Epoch 206, Loss G: -1.8683,Loss D:  0.0990\n",
      "Epoch 207, Loss G: -1.9781,Loss D:  0.0869\n",
      "Epoch 208, Loss G: -2.0433,Loss D:  0.1466\n",
      "Epoch 209, Loss G: -2.1207,Loss D:  0.3644\n",
      "Epoch 210, Loss G: -2.1902,Loss D:  0.2282\n",
      "Epoch 211, Loss G: -2.2995,Loss D:  0.2014\n",
      "Epoch 212, Loss G: -2.3301,Loss D:  0.1780\n",
      "Epoch 213, Loss G: -2.1784,Loss D:  0.2642\n",
      "Epoch 214, Loss G: -2.1753,Loss D:  0.1248\n",
      "Epoch 215, Loss G: -2.0883,Loss D:  0.1478\n",
      "Epoch 216, Loss G: -2.1630,Loss D:  0.0529\n",
      "Epoch 217, Loss G: -2.0875,Loss D: -0.0811\n",
      "Epoch 218, Loss G: -1.9899,Loss D: -0.0380\n",
      "Epoch 219, Loss G: -1.9096,Loss D: -0.1923\n",
      "Epoch 220, Loss G: -1.7320,Loss D: -0.1855\n",
      "Epoch 221, Loss G: -1.8632,Loss D: -0.2743\n",
      "Epoch 222, Loss G: -1.7317,Loss D: -0.3795\n",
      "Epoch 223, Loss G: -1.7628,Loss D: -0.3328\n",
      "Epoch 224, Loss G: -1.7900,Loss D: -0.3302\n",
      "Epoch 225, Loss G: -1.7380,Loss D: -0.2675\n",
      "Epoch 226, Loss G: -1.8896,Loss D: -0.1673\n",
      "Epoch 227, Loss G: -1.9242,Loss D: -0.1399\n",
      "Epoch 228, Loss G: -1.9523,Loss D: -0.1399\n",
      "Epoch 229, Loss G: -1.9512,Loss D: -0.0425\n",
      "Epoch 230, Loss G: -1.9203,Loss D: -0.0664\n",
      "Epoch 231, Loss G: -2.0862,Loss D:  0.1268\n",
      "Epoch 232, Loss G: -1.9222,Loss D:  0.1199\n",
      "Epoch 233, Loss G: -2.0598,Loss D: -0.1008\n",
      "Epoch 234, Loss G: -2.0335,Loss D:  0.0738\n",
      "Epoch 235, Loss G: -2.0568,Loss D:  0.0952\n",
      "Epoch 236, Loss G: -2.0547,Loss D:  0.0464\n",
      "Epoch 237, Loss G: -2.1093,Loss D:  0.1585\n",
      "Epoch 238, Loss G: -2.1367,Loss D:  0.1514\n",
      "Epoch 239, Loss G: -2.0459,Loss D:  0.1866\n",
      "Epoch 240, Loss G: -2.1851,Loss D:  0.2002\n",
      "Epoch 241, Loss G: -2.1855,Loss D:  0.3603\n",
      "Epoch 242, Loss G: -2.1106,Loss D:  0.2630\n",
      "Epoch 243, Loss G: -2.2411,Loss D:  0.1265\n",
      "Epoch 244, Loss G: -2.2914,Loss D:  0.1751\n",
      "Epoch 245, Loss G: -2.1231,Loss D:  0.0962\n",
      "Epoch 246, Loss G: -2.0779,Loss D:  0.0011\n",
      "Epoch 247, Loss G: -1.9988,Loss D:  0.1580\n",
      "Epoch 248, Loss G: -2.2361,Loss D:  0.0374\n",
      "Epoch 249, Loss G: -2.1468,Loss D:  0.0982\n",
      "Epoch 250, Loss G: -2.0072,Loss D: -0.0121\n",
      "Epoch 251, Loss G: -2.2172,Loss D:  0.0301\n",
      "Epoch 252, Loss G: -2.0843,Loss D: -0.0060\n",
      "Epoch 253, Loss G: -2.0886,Loss D:  0.1980\n",
      "Epoch 254, Loss G: -2.2784,Loss D:  0.0266\n",
      "Epoch 255, Loss G: -2.2473,Loss D:  0.1760\n",
      "Epoch 256, Loss G: -2.1584,Loss D:  0.0712\n",
      "Epoch 257, Loss G: -2.1241,Loss D:  0.0907\n",
      "Epoch 258, Loss G: -2.1423,Loss D:  0.0971\n",
      "Epoch 259, Loss G: -2.1699,Loss D:  0.0039\n",
      "Epoch 260, Loss G: -2.1342,Loss D:  0.0656\n",
      "Epoch 261, Loss G: -2.1094,Loss D: -0.0740\n",
      "Epoch 262, Loss G: -1.9731,Loss D: -0.0801\n",
      "Epoch 263, Loss G: -2.0253,Loss D: -0.1966\n",
      "Epoch 264, Loss G: -2.0326,Loss D: -0.0551\n",
      "Epoch 265, Loss G: -2.0626,Loss D: -0.2281\n",
      "Epoch 266, Loss G: -2.0619,Loss D: -0.3195\n",
      "Epoch 267, Loss G: -2.0685,Loss D: -0.1592\n",
      "Epoch 268, Loss G: -1.9932,Loss D: -0.1215\n",
      "Epoch 269, Loss G: -2.0002,Loss D: -0.1868\n",
      "Epoch 270, Loss G: -2.0254,Loss D: -0.0967\n",
      "Epoch 271, Loss G: -1.9847,Loss D:  0.0092\n",
      "Epoch 272, Loss G: -1.8770,Loss D: -0.0412\n",
      "Epoch 273, Loss G: -2.0019,Loss D: -0.0708\n",
      "Epoch 274, Loss G: -2.0003,Loss D:  0.0590\n",
      "Epoch 275, Loss G: -2.1172,Loss D:  0.1043\n",
      "Epoch 276, Loss G: -2.2063,Loss D: -0.0126\n",
      "Epoch 277, Loss G: -2.2780,Loss D:  0.0347\n",
      "Epoch 278, Loss G: -2.3139,Loss D:  0.1417\n",
      "Epoch 279, Loss G: -2.2844,Loss D: -0.0859\n",
      "Epoch 280, Loss G: -2.4130,Loss D:  0.2474\n",
      "Epoch 281, Loss G: -2.3884,Loss D:  0.3260\n",
      "Epoch 282, Loss G: -2.3066,Loss D:  0.2357\n",
      "Epoch 283, Loss G: -2.3312,Loss D:  0.0944\n",
      "Epoch 284, Loss G: -2.3924,Loss D:  0.2255\n",
      "Epoch 285, Loss G: -2.3328,Loss D:  0.1337\n",
      "Epoch 286, Loss G: -2.2551,Loss D:  0.1278\n",
      "Epoch 287, Loss G: -2.2237,Loss D: -0.0122\n",
      "Epoch 288, Loss G: -2.1180,Loss D: -0.1218\n",
      "Epoch 289, Loss G: -2.0176,Loss D:  0.0561\n",
      "Epoch 290, Loss G: -2.0564,Loss D:  0.0948\n",
      "Epoch 291, Loss G: -1.9430,Loss D: -0.1087\n",
      "Epoch 292, Loss G: -1.8538,Loss D: -0.1845\n",
      "Epoch 293, Loss G: -1.8283,Loss D: -0.2259\n",
      "Epoch 294, Loss G: -1.8793,Loss D: -0.2714\n",
      "Epoch 295, Loss G: -1.8290,Loss D: -0.3289\n",
      "Epoch 296, Loss G: -1.7818,Loss D: -0.3055\n",
      "Epoch 297, Loss G: -1.8135,Loss D: -0.1107\n",
      "Epoch 298, Loss G: -1.8806,Loss D: -0.1757\n",
      "Epoch 299, Loss G: -1.8046,Loss D:  0.0381\n",
      "Epoch 300, Loss G: -1.8203,Loss D: -0.1385\n",
      "Epoch 301, Loss G: -1.9146,Loss D: -0.0936\n",
      "Epoch 302, Loss G: -1.7939,Loss D:  0.0053\n",
      "Epoch 303, Loss G: -1.9592,Loss D: -0.2247\n",
      "Epoch 304, Loss G: -1.9856,Loss D: -0.0071\n",
      "Epoch 305, Loss G: -2.0494,Loss D:  0.0052\n",
      "Epoch 306, Loss G: -2.0719,Loss D: -0.0030\n",
      "Epoch 307, Loss G: -2.1514,Loss D:  0.0914\n",
      "Epoch 308, Loss G: -1.9442,Loss D: -0.0002\n",
      "Epoch 309, Loss G: -2.0322,Loss D: -0.0619\n",
      "Epoch 310, Loss G: -2.0654,Loss D:  0.0650\n",
      "Epoch 311, Loss G: -2.0698,Loss D:  0.1895\n",
      "Epoch 312, Loss G: -2.0856,Loss D: -0.1385\n",
      "Epoch 313, Loss G: -2.1265,Loss D:  0.0364\n",
      "Epoch 314, Loss G: -2.0856,Loss D:  0.2294\n",
      "Epoch 315, Loss G: -2.0415,Loss D: -0.0974\n",
      "Epoch 316, Loss G: -2.2023,Loss D:  0.2132\n",
      "Epoch 317, Loss G: -2.1423,Loss D: -0.0197\n",
      "Epoch 318, Loss G: -2.1669,Loss D:  0.1556\n",
      "Epoch 319, Loss G: -2.1473,Loss D: -0.0566\n",
      "Epoch 320, Loss G: -2.1547,Loss D: -0.0226\n",
      "Epoch 321, Loss G: -2.2385,Loss D: -0.0093\n",
      "Epoch 322, Loss G: -2.3538,Loss D: -0.0254\n",
      "Epoch 323, Loss G: -2.4682,Loss D:  0.0525\n",
      "Epoch 324, Loss G: -2.4705,Loss D:  0.0132\n",
      "Epoch 325, Loss G: -2.3081,Loss D:  0.0936\n",
      "Epoch 326, Loss G: -2.3069,Loss D:  0.0507\n",
      "Epoch 327, Loss G: -2.3828,Loss D:  0.0090\n",
      "Epoch 328, Loss G: -2.3293,Loss D: -0.0301\n",
      "Epoch 329, Loss G: -2.1828,Loss D: -0.0141\n",
      "Epoch 330, Loss G: -2.1871,Loss D: -0.0120\n",
      "Epoch 331, Loss G: -2.1837,Loss D: -0.1480\n",
      "Epoch 332, Loss G: -2.2560,Loss D: -0.2830\n",
      "Epoch 333, Loss G: -2.0409,Loss D: -0.1127\n",
      "Epoch 334, Loss G: -1.9197,Loss D: -0.1039\n",
      "Epoch 335, Loss G: -2.0612,Loss D: -0.1135\n",
      "Epoch 336, Loss G: -2.0189,Loss D: -0.1348\n",
      "Epoch 337, Loss G: -1.9893,Loss D: -0.0586\n",
      "Epoch 338, Loss G: -1.9433,Loss D: -0.0334\n",
      "Epoch 339, Loss G: -1.9629,Loss D: -0.1190\n",
      "Epoch 340, Loss G: -1.9507,Loss D: -0.0632\n",
      "Epoch 341, Loss G: -1.8748,Loss D: -0.1514\n",
      "Epoch 342, Loss G: -1.9933,Loss D:  0.0428\n",
      "Epoch 343, Loss G: -2.0652,Loss D: -0.0524\n",
      "Epoch 344, Loss G: -2.0944,Loss D:  0.0340\n",
      "Epoch 345, Loss G: -1.9978,Loss D:  0.1181\n",
      "Epoch 346, Loss G: -2.0557,Loss D: -0.0543\n",
      "Epoch 347, Loss G: -1.9350,Loss D: -0.0222\n",
      "Epoch 348, Loss G: -2.1058,Loss D: -0.0923\n",
      "Epoch 349, Loss G: -2.0060,Loss D:  0.1731\n",
      "Epoch 350, Loss G: -1.8851,Loss D:  0.0980\n",
      "Epoch 351, Loss G: -1.9184,Loss D: -0.0025\n",
      "Epoch 352, Loss G: -1.9229,Loss D:  0.1648\n",
      "Epoch 353, Loss G: -1.9447,Loss D:  0.0273\n",
      "Epoch 354, Loss G: -2.0281,Loss D: -0.1696\n",
      "Epoch 355, Loss G: -1.9444,Loss D:  0.0166\n",
      "Epoch 356, Loss G: -1.9119,Loss D: -0.0069\n",
      "Epoch 357, Loss G: -1.7862,Loss D:  0.0383\n",
      "Epoch 358, Loss G: -1.8960,Loss D: -0.0975\n",
      "Epoch 359, Loss G: -1.8248,Loss D:  0.0247\n",
      "Epoch 360, Loss G: -1.9382,Loss D: -0.0483\n",
      "Epoch 361, Loss G: -1.8219,Loss D:  0.0007\n",
      "Epoch 362, Loss G: -1.8285,Loss D: -0.1238\n",
      "Epoch 363, Loss G: -1.9110,Loss D:  0.0289\n",
      "Epoch 364, Loss G: -1.8157,Loss D:  0.0002\n",
      "Epoch 365, Loss G: -1.9012,Loss D: -0.0461\n",
      "Epoch 366, Loss G: -1.7702,Loss D: -0.0094\n",
      "Epoch 367, Loss G: -1.8080,Loss D:  0.0473\n",
      "Epoch 368, Loss G: -1.7787,Loss D: -0.0571\n",
      "Epoch 369, Loss G: -1.8920,Loss D: -0.1605\n",
      "Epoch 370, Loss G: -1.7529,Loss D: -0.0038\n",
      "Epoch 371, Loss G: -1.9268,Loss D: -0.0197\n",
      "Epoch 372, Loss G: -1.8158,Loss D: -0.1678\n",
      "Epoch 373, Loss G: -1.8856,Loss D: -0.1003\n",
      "Epoch 374, Loss G: -1.8643,Loss D: -0.1521\n",
      "Epoch 375, Loss G: -1.8848,Loss D: -0.0239\n",
      "Epoch 376, Loss G: -1.7875,Loss D:  0.0262\n",
      "Epoch 377, Loss G: -1.9465,Loss D:  0.0604\n",
      "Epoch 378, Loss G: -1.9739,Loss D: -0.0263\n",
      "Epoch 379, Loss G: -1.9436,Loss D: -0.1010\n",
      "Epoch 380, Loss G: -2.0954,Loss D: -0.1537\n",
      "Epoch 381, Loss G: -1.8729,Loss D: -0.0001\n",
      "Epoch 382, Loss G: -2.0453,Loss D: -0.0166\n",
      "Epoch 383, Loss G: -1.9752,Loss D: -0.1644\n",
      "Epoch 384, Loss G: -1.9237,Loss D:  0.0470\n",
      "Epoch 385, Loss G: -2.0831,Loss D:  0.0203\n",
      "Epoch 386, Loss G: -1.8769,Loss D: -0.0165\n",
      "Epoch 387, Loss G: -1.9318,Loss D:  0.0391\n",
      "Epoch 388, Loss G: -1.9845,Loss D: -0.0845\n",
      "Epoch 389, Loss G: -2.0178,Loss D:  0.0048\n",
      "Epoch 390, Loss G: -2.0188,Loss D:  0.0590\n",
      "Epoch 391, Loss G: -2.0055,Loss D: -0.0856\n",
      "Epoch 392, Loss G: -2.0338,Loss D:  0.0805\n",
      "Epoch 393, Loss G: -1.8333,Loss D: -0.2507\n",
      "Epoch 394, Loss G: -1.8968,Loss D: -0.0715\n",
      "Epoch 395, Loss G: -1.8821,Loss D: -0.1556\n",
      "Epoch 396, Loss G: -1.8215,Loss D: -0.1632\n",
      "Epoch 397, Loss G: -1.8727,Loss D: -0.1740\n",
      "Epoch 398, Loss G: -1.8049,Loss D: -0.0561\n",
      "Epoch 399, Loss G: -1.8211,Loss D: -0.2440\n",
      "Epoch 400, Loss G: -1.7346,Loss D: -0.0937\n",
      "Epoch 401, Loss G: -1.8885,Loss D: -0.2724\n",
      "Epoch 402, Loss G: -1.7904,Loss D: -0.0903\n",
      "Epoch 403, Loss G: -1.8742,Loss D: -0.1464\n",
      "Epoch 404, Loss G: -1.8130,Loss D: -0.1883\n",
      "Epoch 405, Loss G: -1.9175,Loss D: -0.0869\n",
      "Epoch 406, Loss G: -1.8465,Loss D: -0.0730\n",
      "Epoch 407, Loss G: -1.8722,Loss D: -0.0475\n",
      "Epoch 408, Loss G: -1.8753,Loss D:  0.1201\n",
      "Epoch 409, Loss G: -1.8700,Loss D: -0.0374\n",
      "Epoch 410, Loss G: -1.8421,Loss D: -0.0536\n",
      "Epoch 411, Loss G: -1.8528,Loss D: -0.0750\n",
      "Epoch 412, Loss G: -1.8699,Loss D:  0.0236\n",
      "Epoch 413, Loss G: -1.9574,Loss D:  0.0157\n",
      "Epoch 414, Loss G: -1.9081,Loss D: -0.0977\n",
      "Epoch 415, Loss G: -1.8664,Loss D:  0.1120\n",
      "Epoch 416, Loss G: -1.9371,Loss D: -0.0367\n",
      "Epoch 417, Loss G: -1.8669,Loss D:  0.1948\n",
      "Epoch 418, Loss G: -1.9192,Loss D: -0.0525\n",
      "Epoch 419, Loss G: -1.8803,Loss D:  0.0114\n",
      "Epoch 420, Loss G: -1.8196,Loss D: -0.0689\n",
      "Epoch 421, Loss G: -1.9807,Loss D:  0.0381\n",
      "Epoch 422, Loss G: -1.7514,Loss D: -0.0698\n",
      "Epoch 423, Loss G: -1.8014,Loss D: -0.0984\n",
      "Epoch 424, Loss G: -1.7508,Loss D: -0.1928\n",
      "Epoch 425, Loss G: -1.6286,Loss D: -0.2857\n",
      "Epoch 426, Loss G: -1.6328,Loss D: -0.2366\n",
      "Epoch 427, Loss G: -1.6869,Loss D: -0.2254\n",
      "Epoch 428, Loss G: -1.6879,Loss D: -0.2039\n",
      "Epoch 429, Loss G: -1.6701,Loss D: -0.2443\n",
      "Epoch 430, Loss G: -1.5555,Loss D: -0.1550\n",
      "Epoch 431, Loss G: -1.7063,Loss D: -0.2976\n",
      "Epoch 432, Loss G: -1.6029,Loss D: -0.0793\n",
      "Epoch 433, Loss G: -1.7152,Loss D: -0.1696\n",
      "Epoch 434, Loss G: -1.6075,Loss D: -0.0664\n",
      "Epoch 435, Loss G: -1.7579,Loss D: -0.1615\n",
      "Epoch 436, Loss G: -1.7707,Loss D: -0.0517\n",
      "Epoch 437, Loss G: -1.7256,Loss D: -0.2272\n",
      "Epoch 438, Loss G: -1.7989,Loss D: -0.0519\n",
      "Epoch 439, Loss G: -1.7451,Loss D:  0.0195\n",
      "Epoch 440, Loss G: -1.7989,Loss D:  0.0627\n",
      "Epoch 441, Loss G: -1.9324,Loss D: -0.0481\n",
      "Epoch 442, Loss G: -1.8564,Loss D:  0.0904\n",
      "Epoch 443, Loss G: -1.8149,Loss D: -0.0522\n",
      "Epoch 444, Loss G: -1.7938,Loss D:  0.0665\n",
      "Epoch 445, Loss G: -1.8412,Loss D: -0.1556\n",
      "Epoch 446, Loss G: -1.7247,Loss D: -0.0421\n",
      "Epoch 447, Loss G: -1.7692,Loss D: -0.0817\n",
      "Epoch 448, Loss G: -1.8404,Loss D: -0.2344\n",
      "Epoch 449, Loss G: -1.7272,Loss D: -0.0768\n",
      "Epoch 450, Loss G: -1.7582,Loss D: -0.1585\n",
      "Epoch 451, Loss G: -1.7066,Loss D: -0.0401\n",
      "Epoch 452, Loss G: -1.6750,Loss D: -0.1417\n",
      "Epoch 453, Loss G: -1.5913,Loss D: -0.2018\n",
      "Epoch 454, Loss G: -1.6828,Loss D:  0.0357\n",
      "Epoch 455, Loss G: -1.6467,Loss D:  0.0798\n",
      "Epoch 456, Loss G: -1.6950,Loss D: -0.2040\n",
      "Epoch 457, Loss G: -1.5798,Loss D: -0.2065\n",
      "Epoch 458, Loss G: -1.6337,Loss D: -0.2116\n",
      "Epoch 459, Loss G: -1.6134,Loss D: -0.2633\n",
      "Epoch 460, Loss G: -1.6068,Loss D: -0.0749\n",
      "Epoch 461, Loss G: -1.7225,Loss D: -0.2337\n",
      "Epoch 462, Loss G: -1.5475,Loss D: -0.1343\n",
      "Epoch 463, Loss G: -1.7452,Loss D:  0.0045\n",
      "Epoch 464, Loss G: -1.5858,Loss D: -0.1335\n",
      "Epoch 465, Loss G: -1.6784,Loss D: -0.0596\n",
      "Epoch 466, Loss G: -1.6428,Loss D: -0.1018\n",
      "Epoch 467, Loss G: -1.6567,Loss D: -0.1694\n",
      "Epoch 468, Loss G: -1.5726,Loss D: -0.1145\n",
      "Epoch 469, Loss G: -1.7315,Loss D: -0.0995\n",
      "Epoch 470, Loss G: -1.6887,Loss D: -0.1541\n",
      "Epoch 471, Loss G: -1.6174,Loss D:  0.0317\n",
      "Epoch 472, Loss G: -1.6864,Loss D: -0.0597\n",
      "Epoch 473, Loss G: -1.7465,Loss D: -0.2566\n",
      "Epoch 474, Loss G: -1.6365,Loss D: -0.0496\n",
      "Epoch 475, Loss G: -1.8146,Loss D: -0.2972\n",
      "Epoch 476, Loss G: -1.6594,Loss D: -0.0445\n",
      "Epoch 477, Loss G: -1.7266,Loss D: -0.0141\n",
      "Epoch 478, Loss G: -1.8160,Loss D: -0.2132\n",
      "Epoch 479, Loss G: -1.7738,Loss D: -0.0250\n",
      "Epoch 480, Loss G: -1.7685,Loss D: -0.0489\n",
      "Epoch 481, Loss G: -1.6903,Loss D:  0.0810\n",
      "Epoch 482, Loss G: -1.8576,Loss D:  0.0276\n",
      "Epoch 483, Loss G: -1.7257,Loss D: -0.1431\n",
      "Epoch 484, Loss G: -1.7313,Loss D: -0.2578\n",
      "Epoch 485, Loss G: -1.6847,Loss D: -0.1746\n",
      "Epoch 486, Loss G: -1.7036,Loss D: -0.2860\n",
      "Epoch 487, Loss G: -1.6465,Loss D: -0.2979\n",
      "Epoch 488, Loss G: -1.6961,Loss D: -0.2176\n",
      "Epoch 489, Loss G: -1.5080,Loss D: -0.3374\n",
      "Epoch 490, Loss G: -1.5038,Loss D: -0.3036\n",
      "Epoch 491, Loss G: -1.5849,Loss D: -0.0962\n",
      "Epoch 492, Loss G: -1.5573,Loss D: -0.2739\n",
      "Epoch 493, Loss G: -1.6479,Loss D: -0.0727\n",
      "Epoch 494, Loss G: -1.5562,Loss D: -0.1628\n",
      "Epoch 495, Loss G: -1.7199,Loss D: -0.1685\n",
      "Epoch 496, Loss G: -1.7249,Loss D:  0.1062\n",
      "Epoch 497, Loss G: -1.6864,Loss D: -0.1990\n",
      "Epoch 498, Loss G: -1.7120,Loss D:  0.0618\n",
      "Epoch 499, Loss G: -1.6406,Loss D: -0.1267\n",
      "Epoch 500, Loss G: -1.8466,Loss D:  0.0030\n"
     ]
    }
   ],
   "source": [
    "# Create fake data via CTGAN\n",
    "fake_data = make_fake_ctgan(meta, 500, real_data, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating report:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating report: 100%|██████████| 4/4 [00:01<00:00,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Quality Score: 86.88%\n",
      "\n",
      "Properties:\n",
      "Column Shapes: 86.11%\n",
      "Column Pair Trends: 87.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fake_data_quality(real_data, fake_data, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifier F1-scores and their Jaccard similarities::\n",
      "                             f1_real  f1_fake  jaccard_similarity\n",
      "index                                                            \n",
      "DecisionTreeClassifier_fake   0.1250   0.2083              0.0435\n",
      "DecisionTreeClassifier_real   0.9583   0.2917              0.2000\n",
      "LogisticRegression_fake       0.1667   0.1250              0.1429\n",
      "LogisticRegression_real       0.9583   0.0833              0.0667\n",
      "MLPClassifier_fake            0.1250   0.2083              0.0435\n",
      "MLPClassifier_real            0.9583   0.0833              0.0435\n",
      "RandomForestClassifier_fake   0.1250   0.1667              0.1163\n",
      "RandomForestClassifier_real   1.0000   0.0417              0.0213\n",
      "\n",
      "Privacy results:\n",
      "                                          result\n",
      "Duplicate rows between sets (real/fake)  (93, 0)\n",
      "nearest neighbor mean                     3.2383\n",
      "nearest neighbor std                      0.5325\n",
      "\n",
      "Miscellaneous results:\n",
      "                                  Result\n",
      "Column Correlation Distance RMSE  0.2983\n",
      "Column Correlation distance MAE   0.2305\n",
      "\n",
      "Results:\n",
      "                                                result\n",
      "Basic statistics                                0.8654\n",
      "Correlation column correlations                -0.0202\n",
      "Mean Correlation between fake and real columns  0.8978\n",
      "1 - MAPE Estimator results                      0.3254\n",
      "Similarity Score                                0.5171\n"
     ]
    }
   ],
   "source": [
    "fake_data_table_evaluator(real_data, fake_data, 'CLASS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Success!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_data(fake_data, 'anx_ep500_1x_test.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for NN Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDependencies\\n- Numpy\\n- Pandas\\n- Tensorflow\\n- Scikit-learn\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dependencies\n",
    "- Numpy\n",
    "- Pandas\n",
    "- Tensorflow\n",
    "- Scikit-learn\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def combine_real_fake_data(real_data_path, fake_data_path):\n",
    "    \"\"\"\n",
    "        Combine real and fake data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        real_data_path : str\n",
    "            Path to real data\n",
    "        fake_data_path : str\n",
    "            Path to fake data\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        DataFrame\n",
    "            Combine real and fake data in Pandas DataFrame\n",
    "    \"\"\"\n",
    "    # Load Data\n",
    "    real = pd.read_csv(real_data_path)\n",
    "    fake = pd.read_csv(fake_data_path)\n",
    "\n",
    "    # Combine\n",
    "    frames = [real, fake]\n",
    "    combine = pd.concat(frames)\n",
    "\n",
    "    return combine\n",
    "\n",
    "def data_preprocessing(combine_data):\n",
    "    \"\"\"\n",
    "        Prepare the data\n",
    "\n",
    "        Parameter\n",
    "        ----------\n",
    "        combine_data : DataFrame\n",
    "            combined real and fake data\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        X features\n",
    "            numpy array in ndarray format\n",
    "        y labels\n",
    "            numpy array in ndarray format\n",
    "    \"\"\"\n",
    "\n",
    "    # Select features and labels\n",
    "    X = combine_data.iloc[:,0:14].values\n",
    "    y = combine_data.iloc[:,-1].values\n",
    "\n",
    "    # Make y in 2 dimension array\n",
    "    y_encode = y.reshape(y.size, 1)\n",
    "\n",
    "    # Define One Hot Encoder Object\n",
    "    ohe = OneHotEncoder()\n",
    "\n",
    "    # Transform y\n",
    "    y_encode = ohe.fit_transform(y_encode)\n",
    "\n",
    "    # Transform to y to ndarray\n",
    "    y_encode = y_encode.toarray()\n",
    "\n",
    "    return X, y_encode\n",
    "\n",
    "def split_data(X, y, test_size=0.2, random_state=0, stratify=None):\n",
    "    \"\"\"\n",
    "        Perform random split train test using stratify method. Default split 80:20\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray\n",
    "            Features vector\n",
    "        y : ndarray\n",
    "            Labels\n",
    "        test_size : float\n",
    "            Test size ratio. Default=0.2\n",
    "        random_state : int\n",
    "            Randomize during data spliting. Default=0\n",
    "        stratify : array-like\n",
    "            Perform sampling using stratify method. Default=None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_train : ndarray\n",
    "            X features for training\n",
    "        X_test : ndarray\n",
    "            X features for testing\n",
    "        y_train : ndarray\n",
    "            y labels for training\n",
    "        y_test : ndarray\n",
    "            y labels for testing\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=stratify)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def eval_nn(X_train, y_train, X_test, y_test, epochs, dense_1_activation='relu', dense_2_activation='relu', output_activation='softmax', optimizer='adam', loss='categorical_crossentropy', metrics='accuracy'):\n",
    "    \n",
    "    # Build ANN\n",
    "    ann = Sequential()\n",
    "    ann.add(Dense(28, activation=dense_1_activation, input_dim=14))\n",
    "    ann.add(Dense(28, activation=dense_2_activation))\n",
    "    ann.add(Dense(6, activation=output_activation))\n",
    "    ann.compile(optimizer=optimizer, loss=loss, metrics=[metrics])\n",
    "\n",
    "    # Fitting\n",
    "    ann.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "    # Evaluate\n",
    "    _, train_acc = ann.evaluate(X_train, y_train, verbose=0)\n",
    "    _, test_acc = ann.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check NN Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine real and fake\n",
    "combine = combine_real_fake_data('../anxiety_class_encoded.csv', 'anx_ep500_1x_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 15)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprosessing\n",
    "X, y = data_preprocessing(combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 14)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 6)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = split_data(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 14)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 6)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.990, Test: 0.438\n"
     ]
    }
   ],
   "source": [
    "eval_nn(X_train, y_train, X_test, y_test, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
